# Outline for future posts and introduction to Transformer models

This blog is created in parallel to my research project in biomedical relation extraction (RE).
Therefore, all posts will be written with this particular use case in mind.
Nevertheless, I will start with a rather general introduction into language models, before I will focus on RE.

The final goal will be to develop a model to perform RE on the [DrugProt dataset](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/track-1/)
from the [BioCreative VII Challenge](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/). 
To get there let us first look at the model on which most recent Language Models are based on: the Transformer model and its self-attention layers.
Afterwards I will give an overview about the evolution of the **Transformer** model and how it gets adapted for the use in bioinformatics.

## The Transformer model

The Transformer model was introduced by Vaswani et. al in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).
To understand it we have to understand its central underlying building block first: **attention** or more specifically **self-attention**.

### Attention & Self-Attention

The Transformer uses self-attention in each of its layers.
I won’t touch on the general attention mechanism here because the Transformer doesn’t use it.
You can find a very accessible and visual explanation of it from [Jay Alammar](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/).

As mentioned the self-attention mechanism is the underlying building block of the multi-head self-attention layers of the Transformer.
Self-attention, also called "Scaled Dot-Product Attention" by Vaswani et. al, allows the model to take the context of a given input token into account.
This matters because often the context of a word or token can influence its meaning.
This leads to a representation of each token, which is a weighted sum of all input tokens of the current input. (Or more specifically: Each output representation is a weighted sum of all input representations, in case of later attention layers)

Mathematically, this boils down to computations between three vectors, a query $q$, a key $k$ and a value vector $v$.
These vectors get computed for each input token as a product of the input token representation and the corresponding weight matrix: $W^Q$, $W^K$ or $W^V$.
The weight matrices are learned parameters within a self-attention layer.
For each token out of $n$ input tokens the dot product of its query vector $q$ and the key vectors $k$ get computed. The $n$ resulting scalar dot products each get scaled (divided) by the size of the vectors $k$.
After applying the softmax function over all scaled products we have the weights for computing the weighted sum of all value vectors.
If the weight matrices where trained successfully this results in a meaningful weighting. Meaningful here means, that the value vectors $v$ of important tokens with respect to the meaning of the considered token get a high weight, while unimportant ones are drowned out.

In practice these steps are computed as matrix products of the query matrix $Q$, the key matrix $K$ and the value matrix $V$:
$$
Attention{Q,K,V}=softmax{\frac{QK^T}{\sqrt{d_k}}V
$$

The whole Transformer model is explained in an other illustrated post by [Jay Alammar](https://jalammar.github.io/illustrated-transformer/).

### Differences to Recurrent Neural Networks (RNNs)

## Bert & Co

## Specialized Models for Applications in Bioinformatics
