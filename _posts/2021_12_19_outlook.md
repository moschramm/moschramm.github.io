# Outline for future posts

This blog is created in parallel to my research project in biomedical relation extraction (RE).
Therefore, all posts will be written with this particular use case in mind.
Nevertheless, I will start with a rather general introduction into language models, before I will focus on RE.

The final goal will be to develop a model to perform RE on the [DrugProt dataset](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/track-1/)
from the [BioCreative VII Challenge](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/). 
To get there let us first look at the model on which most recent Language Models are based on: the Transformer model and its self-attention layers.
Afterwards I will give an overview about the evolution of the Transformer model and how it gets adapted for the use in bioinformatics.

## The Transformer model

### Attention & Self-Attention

## Bert & Co

## Specialized Models for Applications in Bioinformatics
