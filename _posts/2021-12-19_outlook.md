# Outline for future posts and introduction to Transformer models

This blog is created in parallel to my research project in biomedical relation extraction (RE).
Therefore, all posts will be written with this particular use case in mind.
Nevertheless, I will start with a rather general introduction into language models, before I will focus on RE.

The final goal will be to develop a model to perform RE on the [DrugProt dataset](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/track-1/)
from the [BioCreative VII Challenge](https://biocreative.bioinformatics.udel.edu/tasks/biocreative-vii/). 
To get there let us first look at the model on which most recent Language Models are based on: the Transformer model and its self-attention layers.
Afterwards I will give an overview about the evolution of the **Transformer** model and how it gets adapted for the use in bioinformatics.

## The Transformer model

The Transformer model was introduced by Vaswani et. al in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).
To understand it we have to understand its central underlying building block first: **attention** or more specifically **self-attention**.

### Attention & Self-Attention

The Transformer uses self-attention in each of its layers.
I won’t touch on the general attention mechanism here because the Transformer doesn’t use it.
You can find a very accessible and visual explanation of it from [Jay Alammar](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/).

As mentioned the self-attention mechanism is the underlying building block of the multi-head self-attention layers of the Transformer.
Self-attention, also called "Scaled Dot-Product Attention" by Vaswani et. al, allows the model to take the context of a given input token into account.
This matters because often the context of a word or token can influence its meaning.
This leads to a representation of each token, which is a weighted sum of all input tokens of the current input. (Or more specifically: Each output representation is a weighted sum of all input representations)

Mathematically, this boils down to computations between three vectors, a query $q$, a key $k$ and a value vector $v$.


The whole Transformer model is explained in an other illustrated post by [Jay Alammar](https://jalammar.github.io/illustrated-transformer/).

### Differences to Recurrent Neural Networks (RNNs)

## Bert & Co

## Specialized Models for Applications in Bioinformatics
